---
title: "Mossy_custom_mrmr"
output: html_document
date: "2024-11-13"
---

# custom TVD mrmr model with MP data

```{r}
library(mrmr)
```

**Load and format data**

```{r}

surveys <- read.csv("/Users/johnimperato/Desktop/Mossy-Pond/data/survey_MossyPond.csv")
captures <- read.csv("/Users/johnimperato/Documents/private_MP_data/captures_MP.csv")

mp_data <- mrmr::clean_data(captures, surveys)
  
```

**Fit the custom model to MP data and summarize**

```{r}

# Fit the custom model using the modified function
custom_mod_MP <- fit_model_custom(mp_custom_data,
                                chains = 3, 
                                iter_sampling = 1000, 
                                iter_warmup = 400)

custom_mod_mp_summary <- custom_mod_MP$m_fit$summary()
View(custom_mod_mp_summary)

write.csv(custom_mod_mp_summary, "/Users/johnimperato/Desktop/Mossy-Pond_companion/mp_custom_mod_sumary.csv")


```

```{r}

abundance_MP_custom <- mrmr::plot_model(custom_mod_MP, "abundance")
recruitment_MP_custom <- mrmr::plot_model(custom_mod_MP, "recruitment")

```

### Model Comparison â€“ custom/classic

#### Generated Quantities

**Create a table comparing the mean and sd generated quantity values for each model**

This table shows **only the mean and sd** estimated value for each generated quantity. It is meant to serve as a quick reference for comparing model output. See the individual model summaries for complete generated quantity summaries.

```{r}
# pull quantities from model fit summary
N_classic <- classic_mod_54188_summary[, c(1,2,4)]
N_custom <- custom_mod_54188_summary[33659:33674, c(1,2,4)]
B_classic <- classic_mod_54188_summary[4221:4225, c(1,2,4)]
B_custom <- custom_mod_54188_summary[4234:4238, c(1,2,4)]
Phi_classic <- classic_mod_54188_summary[4240:4243, c(1,2,4)]
Phi_custom <- custom_mod_54188_summary[4253:4256, c(1,2,4)]
p_classic <- classic_mod_54188_summary[4226:4239, c(1,2,4)]
p_custom <- custom_mod_54188_summary[4239:4252, c(1,2,4)]
  
GenQuant_comparison <- bind_rows(
  left_join(N_classic, N_custom, by = "variable", suffix = c("_classic", "_custom")) %>% mutate(parameter = "abundance"),
  left_join(B_classic, B_custom, by = "variable", suffix = c("_classic", "_custom")) %>% mutate(parameter = "recruitment"),
  left_join(Phi_classic, Phi_custom, by = "variable", suffix = c("_classic", "_custom")) %>% mutate(parameter = "survival"),
  left_join(p_classic, p_custom, by = "variable", suffix = c("_classic", "_custom")) %>% mutate(parameter = "detection"),
) %>% 
  select(variable, parameter, everything())

print(GenQuant_comparison)

# Convert table to grob (graphical object)
table_grob <- tableGrob(GenQuant_comparison)

# Save as an image
ggsave("/Users/johnimperato/Desktop/Mossy-Pond/out/54188_comparison_table.png", plot = table_grob, width = 8, height = 8)

```

**Visually compare abundance and recruitment estimates between models**

Note that axes are not standardized.

```{r}
# visual comparison of abundance and recruitment plots
abund_recruit_comparison_plot_54188 <- ggarrange(classic_annotated, custom_annotated, ncol=2)
print(abund_recruit_comparison_plot_54188)

```

#### Model Fit

Use leave-one-out (LOO) cross-validation, a standard method to calculate the predictive accuracy of Bayesian models, to compare how well the classic and custom models fit the Trapdoor data. The cmdstanr package has built in LOO capabilities. Briefly,

-   LOO cross-validation uses log likelihood values, which now appear in the model summary as generated quantities, to evaluate how well the model predicts each data point when that point is left out of the fitting process. Likelihood, calculated at the individual level, is the probability of observing the data (in this case, an individual frog's capture history) given the model parameters. Log likelihood calculated at the individual level in the generated quantities block is different than the total log likelihood, summed across all individuals and sessions, calculated during model fitting in the model block.

The loo( ) function generates an expected log predictive density (elpd) value for each model. The loo_compare( ) function ranks models based on their elpd values, with the best fit listed first, and calculates the difference in those values. A difference of more than 2x the standard error is generally considered significant in a LOO comparison.

```{r}
loo_classic <- classic_mod_54188$m_fit$loo()

loo_custom <- custom_mod_54188$m_fit$loo()

loo_compare <- loo::loo_compare(loo_classic, loo_custom)

print(loo_compare)

# extract elpd_diff and se_diff for model1
elpd_diff_model1 <- loo_compare["model1", "elpd_diff"]
se_diff_model1 <- loo_compare["model1", "se_diff"]

# calculate significance ratio
result <- abs(elpd_diff_model1 / se_diff_model1)

# interpret
if (abs(result) > 2) {
  cat("The absolute value of elpd_diff / se_diff for model1:", result, "\n")
  cat("The custom model fits the data significantly better than the classic model.\n")
} else {
  cat("The absolute value of elpd_diff / se_diff for model1:", result, "\n")
  cat("The custom model does not fit the data significantly better than the classic model.\n")
}

```
